# live2d-chatgpt-vits
## 项目内容解构及缝合思路
### vits语音合成模型<br>
* 在实际体验以后，vits的语音合成效果只能用差强人意来形容。其模型原理和训练调参在此不做赘述。最后的语音合成阶段可以最后概况到一个类似下方的函数。<br>
`# 文本/模型内部序号/中英文/模型路径/控制路径`<br>
`generateSound(trans, 0, 1, model_path, config_path)`<br>
* 直接生成语音合成出的音频格式文件。注意，模型对于非母语文本效果非常差，尽量使用母语模型。
* 本人对于以上问题的处理方式是在回答以后套一层百度翻译api翻译成日语再丢个日语模型合成
### chatgpt<br>
* openai的api有如下两种方式进行调用：
1. 使用openai库直接提供token和待回答文本
2. 使用requests库按post方式提供token，文本等信息再用json处理回复  
* 其中，方法1速度略快且使用起来更加方面，不需要用json处理回信。另外，openai的api下传速度很慢，50+字的回传甚至要5-10秒，希望以后能改观一点。
### renpy展示交互界面<br>
* renpy本身是一个galgame引擎，但是其用作交互界面和live2d展示平台非常方便，故目前类似项目均使用其作为基础(谁喜欢写UI啊)。其官方文档链接：https://renpy.cn/doc/index.html
* renpy可以内嵌python代码，但是仅限于原生python且安装package方式较麻烦，局限性很大。因此以renpy生成的游戏界面作为主程序显然是不可能的，故需要使用本地服务器作为其和主程序间的中转。
* （本项目把openai api按方法2嵌入到renpy的脚本里了，但是实际上不如放在主程序里用方法一然后再本地传输过来，下面按照方法一讲）
### 本地服务器<br>
* 使用本地socket通信，地址为127.0.0.1，端口选任意不重复的就行。主程序负责接受交互界面输入的回答，然后丢给chatgpt等回答，等到回答后放进vits模型生成音频。最后完成以后返回一个状态标识给交互界面，交互界面开始显示回答，播放语音。在此之前交互界面就一直循环输出固定语句如"稍等"。
* **注意：不知道为什么，vits直接生成的音频文本renpy无法读取并播放。推测是生成的格式不标准，需要使用pydub库进行一次音频格式转换为其他标准格式。pydub库需要安装ffepmg才能正确运行，而ffmpeg又需要把自己的路径丢到环境变量里。真是莫名其妙。**
### 吐槽
* 这项目也就是看起来比较好玩了。自己复现一遍之后就会觉得三个主要方面：vits，gpt，renpy基本上都有点问题。要么效果差要么速度慢或者局限性高，最后效果就是一句话回复50+字数可能要等上十来秒，语音合成也听起来怪怪的。不过最近效果好的语音模型也有不少，如果要重构项目可以考虑升级模型。另外chatgpt的本地部署基本上是不可能的，那就只能希望他们能把api扩个流吧。renpy就不指望增加python扩展性了，人一个gal引擎正常需求哪需要这些玩意啊（草）

> 本项目为b站bvBV1TD4y1E7e8的复现及解构，其原项目git地址为https://github.com/cjyaddone/ChatWaifu
